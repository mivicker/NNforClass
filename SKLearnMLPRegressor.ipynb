{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "from helpel import *\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MLPRegressor in module sklearn.neural_network.multilayer_perceptron:\n",
      "\n",
      "class MLPRegressor(BaseMultilayerPerceptron, sklearn.base.RegressorMixin)\n",
      " |  MLPRegressor(hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n",
      " |  \n",
      " |  Multi-layer Perceptron regressor.\n",
      " |  \n",
      " |  This model optimizes the squared-loss using LBFGS or stochastic gradient\n",
      " |  descent.\n",
      " |  \n",
      " |  .. versionadded:: 0.18\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)\n",
      " |      The ith element represents the number of neurons in the ith\n",
      " |      hidden layer.\n",
      " |  \n",
      " |  activation : {'identity', 'logistic', 'tanh', 'relu'}, default 'relu'\n",
      " |      Activation function for the hidden layer.\n",
      " |  \n",
      " |      - 'identity', no-op activation, useful to implement linear bottleneck,\n",
      " |        returns f(x) = x\n",
      " |  \n",
      " |      - 'logistic', the logistic sigmoid function,\n",
      " |        returns f(x) = 1 / (1 + exp(-x)).\n",
      " |  \n",
      " |      - 'tanh', the hyperbolic tan function,\n",
      " |        returns f(x) = tanh(x).\n",
      " |  \n",
      " |      - 'relu', the rectified linear unit function,\n",
      " |        returns f(x) = max(0, x)\n",
      " |  \n",
      " |  solver : {'lbfgs', 'sgd', 'adam'}, default 'adam'\n",
      " |      The solver for weight optimization.\n",
      " |  \n",
      " |      - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
      " |  \n",
      " |      - 'sgd' refers to stochastic gradient descent.\n",
      " |  \n",
      " |      - 'adam' refers to a stochastic gradient-based optimizer proposed by\n",
      " |        Kingma, Diederik, and Jimmy Ba\n",
      " |  \n",
      " |      Note: The default solver 'adam' works pretty well on relatively\n",
      " |      large datasets (with thousands of training samples or more) in terms of\n",
      " |      both training time and validation score.\n",
      " |      For small datasets, however, 'lbfgs' can converge faster and perform\n",
      " |      better.\n",
      " |  \n",
      " |  alpha : float, optional, default 0.0001\n",
      " |      L2 penalty (regularization term) parameter.\n",
      " |  \n",
      " |  batch_size : int, optional, default 'auto'\n",
      " |      Size of minibatches for stochastic optimizers.\n",
      " |      If the solver is 'lbfgs', the classifier will not use minibatch.\n",
      " |      When set to \"auto\", `batch_size=min(200, n_samples)`\n",
      " |  \n",
      " |  learning_rate : {'constant', 'invscaling', 'adaptive'}, default 'constant'\n",
      " |      Learning rate schedule for weight updates.\n",
      " |  \n",
      " |      - 'constant' is a constant learning rate given by\n",
      " |        'learning_rate_init'.\n",
      " |  \n",
      " |      - 'invscaling' gradually decreases the learning rate ``learning_rate_``\n",
      " |        at each time step 't' using an inverse scaling exponent of 'power_t'.\n",
      " |        effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
      " |  \n",
      " |      - 'adaptive' keeps the learning rate constant to\n",
      " |        'learning_rate_init' as long as training loss keeps decreasing.\n",
      " |        Each time two consecutive epochs fail to decrease training loss by at\n",
      " |        least tol, or fail to increase validation score by at least tol if\n",
      " |        'early_stopping' is on, the current learning rate is divided by 5.\n",
      " |  \n",
      " |      Only used when solver='sgd'.\n",
      " |  \n",
      " |  learning_rate_init : double, optional, default 0.001\n",
      " |      The initial learning rate used. It controls the step-size\n",
      " |      in updating the weights. Only used when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  power_t : double, optional, default 0.5\n",
      " |      The exponent for inverse scaling learning rate.\n",
      " |      It is used in updating effective learning rate when the learning_rate\n",
      " |      is set to 'invscaling'. Only used when solver='sgd'.\n",
      " |  \n",
      " |  max_iter : int, optional, default 200\n",
      " |      Maximum number of iterations. The solver iterates until convergence\n",
      " |      (determined by 'tol') or this number of iterations. For stochastic\n",
      " |      solvers ('sgd', 'adam'), note that this determines the number of epochs\n",
      " |      (how many times each data point will be used), not the number of\n",
      " |      gradient steps.\n",
      " |  \n",
      " |  shuffle : bool, optional, default True\n",
      " |      Whether to shuffle samples in each iteration. Only used when\n",
      " |      solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default None\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  tol : float, optional, default 1e-4\n",
      " |      Tolerance for the optimization. When the loss or score is not improving\n",
      " |      by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n",
      " |      unless ``learning_rate`` is set to 'adaptive', convergence is\n",
      " |      considered to be reached and training stops.\n",
      " |  \n",
      " |  verbose : bool, optional, default False\n",
      " |      Whether to print progress messages to stdout.\n",
      " |  \n",
      " |  warm_start : bool, optional, default False\n",
      " |      When set to True, reuse the solution of the previous\n",
      " |      call to fit as initialization, otherwise, just erase the\n",
      " |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  momentum : float, default 0.9\n",
      " |      Momentum for gradient descent update.  Should be between 0 and 1. Only\n",
      " |      used when solver='sgd'.\n",
      " |  \n",
      " |  nesterovs_momentum : boolean, default True\n",
      " |      Whether to use Nesterov's momentum. Only used when solver='sgd' and\n",
      " |      momentum > 0.\n",
      " |  \n",
      " |  early_stopping : bool, default False\n",
      " |      Whether to use early stopping to terminate training when validation\n",
      " |      score is not improving. If set to true, it will automatically set\n",
      " |      aside 10% of training data as validation and terminate training when\n",
      " |      validation score is not improving by at least ``tol`` for\n",
      " |      ``n_iter_no_change`` consecutive epochs.\n",
      " |      Only effective when solver='sgd' or 'adam'\n",
      " |  \n",
      " |  validation_fraction : float, optional, default 0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if early_stopping is True\n",
      " |  \n",
      " |  beta_1 : float, optional, default 0.9\n",
      " |      Exponential decay rate for estimates of first moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'\n",
      " |  \n",
      " |  beta_2 : float, optional, default 0.999\n",
      " |      Exponential decay rate for estimates of second moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'\n",
      " |  \n",
      " |  epsilon : float, optional, default 1e-8\n",
      " |      Value for numerical stability in adam. Only used when solver='adam'\n",
      " |  \n",
      " |  n_iter_no_change : int, optional, default 10\n",
      " |      Maximum number of epochs to not meet ``tol`` improvement.\n",
      " |      Only effective when solver='sgd' or 'adam'\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  loss_ : float\n",
      " |      The current loss computed with the loss function.\n",
      " |  \n",
      " |  coefs_ : list, length n_layers - 1\n",
      " |      The ith element in the list represents the weight matrix corresponding\n",
      " |      to layer i.\n",
      " |  \n",
      " |  intercepts_ : list, length n_layers - 1\n",
      " |      The ith element in the list represents the bias vector corresponding to\n",
      " |      layer i + 1.\n",
      " |  \n",
      " |  n_iter_ : int,\n",
      " |      The number of iterations the solver has ran.\n",
      " |  \n",
      " |  n_layers_ : int\n",
      " |      Number of layers.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      Number of outputs.\n",
      " |  \n",
      " |  out_activation_ : string\n",
      " |      Name of the output activation function.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  MLPRegressor trains iteratively since at each time step\n",
      " |  the partial derivatives of the loss function with respect to the model\n",
      " |  parameters are computed to update the parameters.\n",
      " |  \n",
      " |  It can also have a regularization term added to the loss function\n",
      " |  that shrinks model parameters to prevent overfitting.\n",
      " |  \n",
      " |  This implementation works with data represented as dense and sparse numpy\n",
      " |  arrays of floating point values.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  Hinton, Geoffrey E.\n",
      " |      \"Connectionist learning procedures.\" Artificial intelligence 40.1\n",
      " |      (1989): 185-234.\n",
      " |  \n",
      " |  Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of\n",
      " |      training deep feedforward neural networks.\" International Conference\n",
      " |      on Artificial Intelligence and Statistics. 2010.\n",
      " |  \n",
      " |  He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level\n",
      " |      performance on imagenet classification.\" arXiv preprint\n",
      " |      arXiv:1502.01852 (2015).\n",
      " |  \n",
      " |  Kingma, Diederik, and Jimmy Ba. \"Adam: A method for stochastic\n",
      " |      optimization.\" arXiv preprint arXiv:1412.6980 (2014).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MLPRegressor\n",
      " |      BaseMultilayerPerceptron\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the multi-layer perceptron model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like, shape (n_samples, n_outputs)\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseMultilayerPerceptron:\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the model to data matrix X and target(s) y.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns a trained MLP model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseMultilayerPerceptron:\n",
      " |  \n",
      " |  partial_fit\n",
      " |      Update the model with a single iteration over the given data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          The target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns a trained MLP model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a\n",
      " |          precomputed kernel matrix instead, shape = (n_samples,\n",
      " |          n_samples_fitted], where n_samples_fitted is the number of\n",
      " |          samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The R2 score used when calling ``score`` on a regressor will use\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with `metrics.r2_score`. This will influence the ``score`` method of\n",
      " |      all the multioutput regressors (except for\n",
      " |      `multioutput.MultiOutputRegressor`). To specify the default value\n",
      " |      manually and avoid the warning, please either call `metrics.r2_score`\n",
      " |      directly or make a custom scorer with `metrics.make_scorer` (the\n",
      " |      built-in scorer ``'r2'`` uses ``multioutput='uniform_average'``).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MLPRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = generate_data()\n",
    "\n",
    "plt.plot(X[:, 0], y, '*')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 25.73650285\n",
      "Iteration 2, loss = 25.38632663\n",
      "Iteration 3, loss = 25.05162969\n",
      "Iteration 4, loss = 24.71692213\n",
      "Iteration 5, loss = 24.39058232\n",
      "Iteration 6, loss = 24.09059150\n",
      "Iteration 7, loss = 23.81062918\n",
      "Iteration 8, loss = 23.53832967\n",
      "Iteration 9, loss = 23.25379886\n",
      "Iteration 10, loss = 22.95496536\n",
      "Iteration 11, loss = 22.63731391\n",
      "Iteration 12, loss = 22.29689919\n",
      "Iteration 13, loss = 21.93402144\n",
      "Iteration 14, loss = 21.55508516\n",
      "Iteration 15, loss = 21.17867350\n",
      "Iteration 16, loss = 20.82240174\n",
      "Iteration 17, loss = 20.46826193\n",
      "Iteration 18, loss = 20.10333759\n",
      "Iteration 19, loss = 19.73134453\n",
      "Iteration 20, loss = 19.34237636\n",
      "Iteration 21, loss = 18.93284255\n",
      "Iteration 22, loss = 18.50114859\n",
      "Iteration 23, loss = 18.04962864\n",
      "Iteration 24, loss = 17.57977385\n",
      "Iteration 25, loss = 17.10302953\n",
      "Iteration 26, loss = 16.61329143\n",
      "Iteration 27, loss = 16.10482407\n",
      "Iteration 28, loss = 15.58144583\n",
      "Iteration 29, loss = 15.04897101\n",
      "Iteration 30, loss = 14.50734756\n",
      "Iteration 31, loss = 13.95596009\n",
      "Iteration 32, loss = 13.39219493\n",
      "Iteration 33, loss = 12.81648171\n",
      "Iteration 34, loss = 12.23159619\n",
      "Iteration 35, loss = 11.63852966\n",
      "Iteration 36, loss = 11.03853677\n",
      "Iteration 37, loss = 10.43697510\n",
      "Iteration 38, loss = 9.83482184\n",
      "Iteration 39, loss = 9.23514336\n",
      "Iteration 40, loss = 8.64118010\n",
      "Iteration 41, loss = 8.05337810\n",
      "Iteration 42, loss = 7.47436833\n",
      "Iteration 43, loss = 6.90687399\n",
      "Iteration 44, loss = 6.35364477\n",
      "Iteration 45, loss = 5.81911263\n",
      "Iteration 46, loss = 5.31360676\n",
      "Iteration 47, loss = 4.83867927\n",
      "Iteration 48, loss = 4.40039678\n",
      "Iteration 49, loss = 4.00194653\n",
      "Iteration 50, loss = 3.64247260\n",
      "Iteration 51, loss = 3.32351132\n",
      "Iteration 52, loss = 3.04579577\n",
      "Iteration 53, loss = 2.81017808\n",
      "Iteration 54, loss = 2.61622723\n",
      "Iteration 55, loss = 2.46130460\n",
      "Iteration 56, loss = 2.34222339\n",
      "Iteration 57, loss = 2.25394308\n",
      "Iteration 58, loss = 2.18985414\n",
      "Iteration 59, loss = 2.14203534\n",
      "Iteration 60, loss = 2.10133646\n",
      "Iteration 61, loss = 2.05965001\n",
      "Iteration 62, loss = 2.01373025\n",
      "Iteration 63, loss = 1.96318475\n",
      "Iteration 64, loss = 1.90671535\n",
      "Iteration 65, loss = 1.84462982\n",
      "Iteration 66, loss = 1.77809774\n",
      "Iteration 67, loss = 1.70900182\n",
      "Iteration 68, loss = 1.63978407\n",
      "Iteration 69, loss = 1.57256483\n",
      "Iteration 70, loss = 1.50963446\n",
      "Iteration 71, loss = 1.45180171\n",
      "Iteration 72, loss = 1.39932368\n",
      "Iteration 73, loss = 1.35227836\n",
      "Iteration 74, loss = 1.30983790\n",
      "Iteration 75, loss = 1.27083273\n",
      "Iteration 76, loss = 1.23366812\n",
      "Iteration 77, loss = 1.19767612\n",
      "Iteration 78, loss = 1.16241022\n",
      "Iteration 79, loss = 1.12754757\n",
      "Iteration 80, loss = 1.09305593\n",
      "Iteration 81, loss = 1.05831128\n",
      "Iteration 82, loss = 1.02340553\n",
      "Iteration 83, loss = 0.98819183\n",
      "Iteration 84, loss = 0.95299447\n",
      "Iteration 85, loss = 0.91803808\n",
      "Iteration 86, loss = 0.88376542\n",
      "Iteration 87, loss = 0.85032380\n",
      "Iteration 88, loss = 0.81774317\n",
      "Iteration 89, loss = 0.78702695\n",
      "Iteration 90, loss = 0.75826212\n",
      "Iteration 91, loss = 0.73129318\n",
      "Iteration 92, loss = 0.70612481\n",
      "Iteration 93, loss = 0.68276264\n",
      "Iteration 94, loss = 0.66103101\n",
      "Iteration 95, loss = 0.64065497\n",
      "Iteration 96, loss = 0.62143419\n",
      "Iteration 97, loss = 0.60320201\n",
      "Iteration 98, loss = 0.58586519\n",
      "Iteration 99, loss = 0.56920051\n",
      "Iteration 100, loss = 0.55329119\n",
      "Iteration 101, loss = 0.53811807\n",
      "Iteration 102, loss = 0.52361170\n",
      "Iteration 103, loss = 0.50969114\n",
      "Iteration 104, loss = 0.49650495\n",
      "Iteration 105, loss = 0.48407009\n",
      "Iteration 106, loss = 0.47243476\n",
      "Iteration 107, loss = 0.46150784\n",
      "Iteration 108, loss = 0.45122739\n",
      "Iteration 109, loss = 0.44166765\n",
      "Iteration 110, loss = 0.43267043\n",
      "Iteration 111, loss = 0.42412464\n",
      "Iteration 112, loss = 0.41602433\n",
      "Iteration 113, loss = 0.40836869\n",
      "Iteration 114, loss = 0.40118799\n",
      "Iteration 115, loss = 0.39439791\n",
      "Iteration 116, loss = 0.38797447\n",
      "Iteration 117, loss = 0.38177981\n",
      "Iteration 118, loss = 0.37586667\n",
      "Iteration 119, loss = 0.36998301\n",
      "Iteration 120, loss = 0.36438840\n",
      "Iteration 121, loss = 0.35914891\n",
      "Iteration 122, loss = 0.35419492\n",
      "Iteration 123, loss = 0.34949584\n",
      "Iteration 124, loss = 0.34499032\n",
      "Iteration 125, loss = 0.34061628\n",
      "Iteration 126, loss = 0.33632276\n",
      "Iteration 127, loss = 0.33213851\n",
      "Iteration 128, loss = 0.32814007\n",
      "Iteration 129, loss = 0.32427771\n",
      "Iteration 130, loss = 0.32057021\n",
      "Iteration 131, loss = 0.31699765\n",
      "Iteration 132, loss = 0.31363622\n",
      "Iteration 133, loss = 0.31047538\n",
      "Iteration 134, loss = 0.30751020\n",
      "Iteration 135, loss = 0.30465214\n",
      "Iteration 136, loss = 0.30190387\n",
      "Iteration 137, loss = 0.29922732\n",
      "Iteration 138, loss = 0.29664632\n",
      "Iteration 139, loss = 0.29414660\n",
      "Iteration 140, loss = 0.29170645\n",
      "Iteration 141, loss = 0.28935937\n",
      "Iteration 142, loss = 0.28713937\n",
      "Iteration 143, loss = 0.28496021\n",
      "Iteration 144, loss = 0.28288068\n",
      "Iteration 145, loss = 0.28088353\n",
      "Iteration 146, loss = 0.27894640\n",
      "Iteration 147, loss = 0.27710359\n",
      "Iteration 148, loss = 0.27531556\n",
      "Iteration 149, loss = 0.27357771\n",
      "Iteration 150, loss = 0.27191428\n",
      "Iteration 151, loss = 0.27029275\n",
      "Iteration 152, loss = 0.26875820\n",
      "Iteration 153, loss = 0.26728833\n",
      "Iteration 154, loss = 0.26588111\n",
      "Iteration 155, loss = 0.26450675\n",
      "Iteration 156, loss = 0.26320038\n",
      "Iteration 157, loss = 0.26193648\n",
      "Iteration 158, loss = 0.26070706\n",
      "Iteration 159, loss = 0.25954500\n",
      "Iteration 160, loss = 0.25839494\n",
      "Iteration 161, loss = 0.25727276\n",
      "Iteration 162, loss = 0.25621557\n",
      "Iteration 163, loss = 0.25517015\n",
      "Iteration 164, loss = 0.25415403\n",
      "Iteration 165, loss = 0.25316859\n",
      "Iteration 166, loss = 0.25218684\n",
      "Iteration 167, loss = 0.25124305\n",
      "Iteration 168, loss = 0.25029535\n",
      "Iteration 169, loss = 0.24936778\n",
      "Iteration 170, loss = 0.24845883\n",
      "Iteration 171, loss = 0.24755776\n",
      "Iteration 172, loss = 0.24665789\n",
      "Iteration 173, loss = 0.24575093\n",
      "Iteration 174, loss = 0.24484612\n",
      "Iteration 175, loss = 0.24392447\n",
      "Iteration 176, loss = 0.24304230\n",
      "Iteration 177, loss = 0.24212905\n",
      "Iteration 178, loss = 0.24123546\n",
      "Iteration 179, loss = 0.24040375\n",
      "Iteration 180, loss = 0.23963329\n",
      "Iteration 181, loss = 0.23863244\n",
      "Iteration 182, loss = 0.23715867\n",
      "Iteration 183, loss = 0.23503287\n",
      "Iteration 184, loss = 0.23244419\n",
      "Iteration 185, loss = 0.22992884\n",
      "Iteration 186, loss = 0.22794935\n",
      "Iteration 187, loss = 0.22656895\n",
      "Iteration 188, loss = 0.22565748\n",
      "Iteration 189, loss = 0.22508667\n",
      "Iteration 190, loss = 0.22469101\n",
      "Iteration 191, loss = 0.22422204\n",
      "Iteration 192, loss = 0.22358347\n",
      "Iteration 193, loss = 0.22273982\n",
      "Iteration 194, loss = 0.22179194\n",
      "Iteration 195, loss = 0.22079658\n",
      "Iteration 196, loss = 0.21983456\n",
      "Iteration 197, loss = 0.21883478\n",
      "Iteration 198, loss = 0.21775854\n",
      "Iteration 199, loss = 0.21660667\n",
      "Iteration 200, loss = 0.21544176\n",
      "Iteration 201, loss = 0.21427481\n",
      "Iteration 202, loss = 0.21313610\n",
      "Iteration 203, loss = 0.21209350\n",
      "Iteration 204, loss = 0.21114081\n",
      "Iteration 205, loss = 0.21021365\n",
      "Iteration 206, loss = 0.20931100\n",
      "Iteration 207, loss = 0.20844814\n",
      "Iteration 208, loss = 0.20755114\n",
      "Iteration 209, loss = 0.20662636\n",
      "Iteration 210, loss = 0.20570810\n",
      "Iteration 211, loss = 0.20487673\n",
      "Iteration 212, loss = 0.20422382\n",
      "Iteration 213, loss = 0.20390232\n",
      "Iteration 214, loss = 0.20340865\n",
      "Iteration 215, loss = 0.20221641\n",
      "Iteration 216, loss = 0.20010241\n",
      "Iteration 217, loss = 0.19845296\n",
      "Iteration 218, loss = 0.19786776\n",
      "Iteration 219, loss = 0.19766136\n",
      "Iteration 220, loss = 0.19690869\n",
      "Iteration 221, loss = 0.19536298\n",
      "Iteration 222, loss = 0.19380459\n",
      "Iteration 223, loss = 0.19291808\n",
      "Iteration 224, loss = 0.19241952\n",
      "Iteration 225, loss = 0.19171568\n",
      "Iteration 226, loss = 0.19040911\n",
      "Iteration 227, loss = 0.18899501\n",
      "Iteration 228, loss = 0.18794975\n",
      "Iteration 229, loss = 0.18726350\n",
      "Iteration 230, loss = 0.18652675\n",
      "Iteration 231, loss = 0.18545650\n",
      "Iteration 232, loss = 0.18416272\n",
      "Iteration 233, loss = 0.18302186\n",
      "Iteration 234, loss = 0.18210278\n",
      "Iteration 235, loss = 0.18130322\n",
      "Iteration 236, loss = 0.18039591\n",
      "Iteration 237, loss = 0.17934275\n",
      "Iteration 238, loss = 0.17817288\n",
      "Iteration 239, loss = 0.17702019\n",
      "Iteration 240, loss = 0.17599897\n",
      "Iteration 241, loss = 0.17507416\n",
      "Iteration 242, loss = 0.17413844\n",
      "Iteration 243, loss = 0.17317424\n",
      "Iteration 244, loss = 0.17215867\n",
      "Iteration 245, loss = 0.17109271\n",
      "Iteration 246, loss = 0.17005805\n",
      "Iteration 247, loss = 0.16897842\n",
      "Iteration 248, loss = 0.16789439\n",
      "Iteration 249, loss = 0.16685491\n",
      "Iteration 250, loss = 0.16581890\n",
      "Iteration 251, loss = 0.16476969\n",
      "Iteration 252, loss = 0.16371150\n",
      "Iteration 253, loss = 0.16267316\n",
      "Iteration 254, loss = 0.16162236\n",
      "Iteration 255, loss = 0.16059677\n",
      "Iteration 256, loss = 0.15959353\n",
      "Iteration 257, loss = 0.15862815\n",
      "Iteration 258, loss = 0.15773768\n",
      "Iteration 259, loss = 0.15700722\n",
      "Iteration 260, loss = 0.15668646\n",
      "Iteration 261, loss = 0.15703148\n",
      "Iteration 262, loss = 0.15797267\n",
      "Iteration 263, loss = 0.15827154\n",
      "Iteration 264, loss = 0.15599643\n",
      "Iteration 265, loss = 0.15185456\n",
      "Iteration 266, loss = 0.14927762\n",
      "Iteration 267, loss = 0.14964274\n",
      "Iteration 268, loss = 0.15055442\n",
      "Iteration 269, loss = 0.14929303\n",
      "Iteration 270, loss = 0.14631146\n",
      "Iteration 271, loss = 0.14445211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 272, loss = 0.14458458\n",
      "Iteration 273, loss = 0.14474655\n",
      "Iteration 274, loss = 0.14317471\n",
      "Iteration 275, loss = 0.14089800\n",
      "Iteration 276, loss = 0.13981730\n",
      "Iteration 277, loss = 0.13977249\n",
      "Iteration 278, loss = 0.13918207\n",
      "Iteration 279, loss = 0.13752203\n",
      "Iteration 280, loss = 0.13592128\n",
      "Iteration 281, loss = 0.13522251\n",
      "Iteration 282, loss = 0.13485130\n",
      "Iteration 283, loss = 0.13394293\n",
      "Iteration 284, loss = 0.13247116\n",
      "Iteration 285, loss = 0.13119953\n",
      "Iteration 286, loss = 0.13048300\n",
      "Iteration 287, loss = 0.12989240\n",
      "Iteration 288, loss = 0.12897095\n",
      "Iteration 289, loss = 0.12770596\n",
      "Iteration 290, loss = 0.12653456\n",
      "Iteration 291, loss = 0.12567751\n",
      "Iteration 292, loss = 0.12496927\n",
      "Iteration 293, loss = 0.12412529\n",
      "Iteration 294, loss = 0.12305984\n",
      "Iteration 295, loss = 0.12194411\n",
      "Iteration 296, loss = 0.12095830\n",
      "Iteration 297, loss = 0.12013297\n",
      "Iteration 298, loss = 0.11932117\n",
      "Iteration 299, loss = 0.11843028\n",
      "Iteration 300, loss = 0.11745484\n",
      "Iteration 301, loss = 0.11644361\n",
      "Iteration 302, loss = 0.11547301\n",
      "Iteration 303, loss = 0.11457347\n",
      "Iteration 304, loss = 0.11372018\n",
      "Iteration 305, loss = 0.11309757\n",
      "Iteration 306, loss = 0.11292304\n",
      "Iteration 307, loss = 0.11314630\n",
      "Iteration 308, loss = 0.11265236\n",
      "Iteration 309, loss = 0.11114331\n",
      "Iteration 310, loss = 0.10912094\n",
      "Iteration 311, loss = 0.10755298\n",
      "Iteration 312, loss = 0.10684958\n",
      "Iteration 313, loss = 0.10663045\n",
      "Iteration 314, loss = 0.10627097\n",
      "Iteration 315, loss = 0.10540424\n",
      "Iteration 316, loss = 0.10405071\n",
      "Iteration 317, loss = 0.10263213\n",
      "Iteration 318, loss = 0.10157130\n",
      "Iteration 319, loss = 0.10093207\n",
      "Iteration 320, loss = 0.10046360\n",
      "Iteration 321, loss = 0.09985642\n",
      "Iteration 322, loss = 0.09896273\n",
      "Iteration 323, loss = 0.09785687\n",
      "Iteration 324, loss = 0.09674006\n",
      "Iteration 325, loss = 0.09578249\n",
      "Iteration 326, loss = 0.09502140\n",
      "Iteration 327, loss = 0.09438031\n",
      "Iteration 328, loss = 0.09374388\n",
      "Iteration 329, loss = 0.09304734\n",
      "Iteration 330, loss = 0.09227799\n",
      "Iteration 331, loss = 0.09141873\n",
      "Iteration 332, loss = 0.09049914\n",
      "Iteration 333, loss = 0.08956608\n",
      "Iteration 334, loss = 0.08867843\n",
      "Iteration 335, loss = 0.08785604\n",
      "Iteration 336, loss = 0.08708893\n",
      "Iteration 337, loss = 0.08637103\n",
      "Iteration 338, loss = 0.08569264\n",
      "Iteration 339, loss = 0.08504394\n",
      "Iteration 340, loss = 0.08440778\n",
      "Iteration 341, loss = 0.08380453\n",
      "Iteration 342, loss = 0.08331453\n",
      "Iteration 343, loss = 0.08288370\n",
      "Iteration 344, loss = 0.08258871\n",
      "Iteration 345, loss = 0.08231060\n",
      "Iteration 346, loss = 0.08202171\n",
      "Iteration 347, loss = 0.08158950\n",
      "Iteration 348, loss = 0.08100108\n",
      "Iteration 349, loss = 0.08004032\n",
      "Iteration 350, loss = 0.07883442\n",
      "Iteration 351, loss = 0.07742039\n",
      "Iteration 352, loss = 0.07609042\n",
      "Iteration 353, loss = 0.07504699\n",
      "Iteration 354, loss = 0.07436424\n",
      "Iteration 355, loss = 0.07397551\n",
      "Iteration 356, loss = 0.07374055\n",
      "Iteration 357, loss = 0.07354439\n",
      "Iteration 358, loss = 0.07326141\n",
      "Iteration 359, loss = 0.07281248\n",
      "Iteration 360, loss = 0.07211780\n",
      "Iteration 361, loss = 0.07119738\n",
      "Iteration 362, loss = 0.07016659\n",
      "Iteration 363, loss = 0.06917027\n",
      "Iteration 364, loss = 0.06833732\n",
      "Iteration 365, loss = 0.06769365\n",
      "Iteration 366, loss = 0.06720455\n",
      "Iteration 367, loss = 0.06681466\n",
      "Iteration 368, loss = 0.06646818\n",
      "Iteration 369, loss = 0.06611352\n",
      "Iteration 370, loss = 0.06572380\n",
      "Iteration 371, loss = 0.06526967\n",
      "Iteration 372, loss = 0.06475367\n",
      "Iteration 373, loss = 0.06417280\n",
      "Iteration 374, loss = 0.06354162\n",
      "Iteration 375, loss = 0.06287683\n",
      "Iteration 376, loss = 0.06219714\n",
      "Iteration 377, loss = 0.06152717\n",
      "Iteration 378, loss = 0.06087963\n",
      "Iteration 379, loss = 0.06026408\n",
      "Iteration 380, loss = 0.05968022\n",
      "Iteration 381, loss = 0.05912575\n",
      "Iteration 382, loss = 0.05859520\n",
      "Iteration 383, loss = 0.05808311\n",
      "Iteration 384, loss = 0.05758571\n",
      "Iteration 385, loss = 0.05709906\n",
      "Iteration 386, loss = 0.05662137\n",
      "Iteration 387, loss = 0.05615413\n",
      "Iteration 388, loss = 0.05570269\n",
      "Iteration 389, loss = 0.05527507\n",
      "Iteration 390, loss = 0.05488527\n",
      "Iteration 391, loss = 0.05456308\n",
      "Iteration 392, loss = 0.05436171\n",
      "Iteration 393, loss = 0.05438012\n",
      "Iteration 394, loss = 0.05477243\n",
      "Iteration 395, loss = 0.05578147\n",
      "Iteration 396, loss = 0.05757004\n",
      "Iteration 397, loss = 0.06001639\n",
      "Iteration 398, loss = 0.06168966\n",
      "Iteration 399, loss = 0.06079780\n",
      "Iteration 400, loss = 0.05618732\n",
      "Iteration 401, loss = 0.05120296\n",
      "Iteration 402, loss = 0.04964115\n",
      "Iteration 403, loss = 0.05167559\n",
      "Iteration 404, loss = 0.05380660\n",
      "Iteration 405, loss = 0.05293555\n",
      "Iteration 406, loss = 0.04986293\n",
      "Iteration 407, loss = 0.04769078\n",
      "Iteration 408, loss = 0.04815330\n",
      "Iteration 409, loss = 0.04955996\n",
      "Iteration 410, loss = 0.04937919\n",
      "Iteration 411, loss = 0.04752221\n",
      "Iteration 412, loss = 0.04595842\n",
      "Iteration 413, loss = 0.04600915\n",
      "Iteration 414, loss = 0.04678955\n",
      "Iteration 415, loss = 0.04671898\n",
      "Iteration 416, loss = 0.04558140\n",
      "Iteration 417, loss = 0.04436637\n",
      "Iteration 418, loss = 0.04404529\n",
      "Iteration 419, loss = 0.04436754\n",
      "Iteration 420, loss = 0.04440500\n",
      "Iteration 421, loss = 0.04376132\n",
      "Iteration 422, loss = 0.04285791\n",
      "Iteration 423, loss = 0.04235244\n",
      "Iteration 424, loss = 0.04233185\n",
      "Iteration 425, loss = 0.04236695\n",
      "Iteration 426, loss = 0.04206750\n",
      "Iteration 427, loss = 0.04146116\n",
      "Iteration 428, loss = 0.04088777\n",
      "Iteration 429, loss = 0.04058795\n",
      "Iteration 430, loss = 0.04049513\n",
      "Iteration 431, loss = 0.04037115\n",
      "Iteration 432, loss = 0.04005291\n",
      "Iteration 433, loss = 0.03959747\n",
      "Iteration 434, loss = 0.03916553\n",
      "Iteration 435, loss = 0.03886969\n",
      "Iteration 436, loss = 0.03868858\n",
      "Iteration 437, loss = 0.03851827\n",
      "Iteration 438, loss = 0.03827171\n",
      "Iteration 439, loss = 0.03793961\n",
      "Iteration 440, loss = 0.03757653\n",
      "Iteration 441, loss = 0.03725034\n",
      "Iteration 442, loss = 0.03699008\n",
      "Iteration 443, loss = 0.03677680\n",
      "Iteration 444, loss = 0.03656759\n",
      "Iteration 445, loss = 0.03632804\n",
      "Iteration 446, loss = 0.03605122\n",
      "Iteration 447, loss = 0.03575382\n",
      "Iteration 448, loss = 0.03546120\n",
      "Iteration 449, loss = 0.03519100\n",
      "Iteration 450, loss = 0.03494662\n",
      "Iteration 451, loss = 0.03471958\n",
      "Iteration 452, loss = 0.03449613\n",
      "Iteration 453, loss = 0.03426530\n",
      "Iteration 454, loss = 0.03402262\n",
      "Iteration 455, loss = 0.03377047\n",
      "Iteration 456, loss = 0.03351458\n",
      "Iteration 457, loss = 0.03326126\n",
      "Iteration 458, loss = 0.03301465\n",
      "Iteration 459, loss = 0.03277643\n",
      "Iteration 460, loss = 0.03254594\n",
      "Iteration 461, loss = 0.03232127\n",
      "Iteration 462, loss = 0.03210036\n",
      "Iteration 463, loss = 0.03188143\n",
      "Iteration 464, loss = 0.03167314\n",
      "Iteration 465, loss = 0.03148450\n",
      "Iteration 466, loss = 0.03131763\n",
      "Iteration 467, loss = 0.03115661\n",
      "Iteration 468, loss = 0.03099995\n",
      "Iteration 469, loss = 0.03084727\n",
      "Iteration 470, loss = 0.03070365\n",
      "Iteration 471, loss = 0.03058712\n",
      "Iteration 472, loss = 0.03049819\n",
      "Iteration 473, loss = 0.03045480\n",
      "Iteration 474, loss = 0.03046542\n",
      "Iteration 475, loss = 0.03055389\n",
      "Iteration 476, loss = 0.03071797\n",
      "Iteration 477, loss = 0.03097669\n",
      "Iteration 478, loss = 0.03128317\n",
      "Iteration 479, loss = 0.03155441\n",
      "Iteration 480, loss = 0.03169887\n",
      "Iteration 481, loss = 0.03156069\n",
      "Iteration 482, loss = 0.03106427\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(100, 100, 100), learning_rate='constant',\n",
       "             learning_rate_init=0.001, max_iter=12000, momentum=0.9,\n",
       "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "             random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "             validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Reg = MLPRegressor(hidden_layer_sizes = (100,100,100), max_iter = 12000, verbose=True)\n",
    "\n",
    "Reg.fit(X[:,0].reshape(-1,1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preds = Reg.predict(X[:,0].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X[:, 0], y)\n",
    "plt.plot(X[:, 0], Preds)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
